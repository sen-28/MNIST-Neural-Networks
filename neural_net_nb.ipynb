{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTASK 2\\n\\nINSTRUCTIONS:\\n\\n* Go through the documentaation of scikit from:\\n  https://scikit-image.org/docs/stable/\\n  focus more on the neural network modules\\n  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html (Neural Network Classifier)\\n  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html  (Neural Network Regressor)\\n\\n* Go through the MNIST dataset given here:\\n  http://yann.lecun.com/exdb/mnist/\\n  It can also be downloaded directly using scikit:\\n  https://scikit-learn.org/0.19/datasets/mldata.html\\n  But this seems to be deprecated, you could use a workaround given here:\\n  https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn\\n\\n* Build a simple neural network (using scikit) and train it to recognize handwritten digits using the MNIST datasetself.\\n  Make sure that you are able to vsualize the different aspects of the network, play around with the hyper-parameters and\\n  try to get the best possible accuracy and report your accuracy on the ML-SIG group / channel\\n  Remember to test different hyper-parameters on the validation set and to report the accuracy from the test set\\n  https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resources:\n",
    "* https://youtu.be/aircAruvnKk\n",
    "* http://neuralnetworksanddeeplearning.com/\n",
    "* playground.tensorflow.org\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 1\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "There are 11 TODOS in this python file\n",
    "Fill each one of those appropriately and you will have a working neural network\n",
    "Instructions and resources have been provided wherever possible.\n",
    "The implementation may not be perfect, so feel free to point out any mistakes / ask any doubts\n",
    "\n",
    "After completing the task, some of the things you could try are (optional):\n",
    "* Implement different cost functions (binary cross-entropy)\n",
    "* Implement different activation functions (tanh, ReLU, softmax)\n",
    "* Incorporate these changes in the neural netwok code so that you can select the loss / activation function\n",
    "* Play with the hyper-paramters!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 2\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "* Go through the documentaation of scikit from:\n",
    "  https://scikit-image.org/docs/stable/\n",
    "  focus more on the neural network modules\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html (Neural Network Classifier)\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html  (Neural Network Regressor)\n",
    "\n",
    "* Go through the MNIST dataset given here:\n",
    "  http://yann.lecun.com/exdb/mnist/\n",
    "  It can also be downloaded directly using scikit:\n",
    "  https://scikit-learn.org/0.19/datasets/mldata.html\n",
    "  But this seems to be deprecated, you could use a workaround given here:\n",
    "  https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn\n",
    "\n",
    "* Build a simple neural network (using scikit) and train it to recognize handwritten digits using the MNIST datasetself.\n",
    "  Make sure that you are able to vsualize the different aspects of the network, play around with the hyper-parameters and\n",
    "  try to get the best possible accuracy and report your accuracy on the ML-SIG group / channel\n",
    "  Remember to test different hyper-parameters on the validation set and to report the accuracy from the test set\n",
    "  https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Other Common activation functions are:\n",
    "* tanh\n",
    "* ReLU\n",
    "* Softmax\n",
    "\n",
    "Read more about these at:\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\"\"\"\n",
    "\n",
    "def activation(z, derivative=False):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function:\n",
    "    It handles two modes: normal and derivative mode.\n",
    "    Applies a pointwise operation on vectors\n",
    "\n",
    "    Parameters:\n",
    "    ---\n",
    "    z: pre-activation vector at layer l\n",
    "        shape (n[l], batch_size)\n",
    "    Returns:\n",
    "    pontwize activation on each element of the input z\n",
    "    \"\"\"\n",
    "    sigmoid = 1/(1 + np.exp(-z))\n",
    "    if derivative:\n",
    "        return sigmoid*(1-sigmoid)\n",
    "        # TODO\n",
    "        # return the derivative of the sigmoid activation function\n",
    "    else:\n",
    "        return sigmoid\n",
    "        # TODO\n",
    "        # return the normal sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Mean Square Error between a ground truth vector and a prediction vector\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost: a scalar value representing the loss\n",
    "    \"\"\"\n",
    "    n = y_pred.shape[1]\n",
    "    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
    "    return cost\n",
    "\n",
    "def cost_function_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the loss function w.r.t the activation of the output layer\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost_prime: derivative of the loss w.r.t. the activation of the output\n",
    "    shape: (n[L], batch_size)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    cost_prime = y_pred - y_true\n",
    "    # Calculate the derivative of the cost function\n",
    "    return cost_prime.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    This is a custom neural netwok package built from scratch with numpy.\n",
    "    The Neural Network as well as its parameters and training method and procedure will\n",
    "    reside in this class.\n",
    "    Parameters\n",
    "    ---\n",
    "    size: list of number of neurons per layer\n",
    "    Examples\n",
    "    ---\n",
    "    >>> import NeuralNetwork\n",
    "    >>> nn = NeuralNetwork([2, 3, 4, 1])\n",
    "\n",
    "    This means :\n",
    "    1 input layer with 2 neurons\n",
    "    1 hidden layer with 3 neurons\n",
    "    1 hidden layer with 4 neurons\n",
    "    1 output layer with 1 neuron\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, seed=42):\n",
    "        '''\n",
    "        Instantiate the weights and biases of the network\n",
    "        weights and biases are attributes of the NeuralNetwork class\n",
    "        They are updated during the training\n",
    "        '''\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
    "        \n",
    "        # TODO\n",
    "        # initialize the weights randomly\n",
    "        \"\"\"\n",
    "        Be careful with the dimensions of the weights\n",
    "        The dimensions of the weight of any particular layer will depend on the\n",
    "        size of the current layer and the previous layer\n",
    "        Example: Size = [16,8,4,2]\n",
    "        The weight file will be a list with 3 matrices with shapes:\n",
    "        (8,16) for weights connecting layers 1 (16) and 2(8)\n",
    "        (4,8) for weights connecting layers 2 (8) and 4(4)\n",
    "        (2,4) for weights connecting layers 3 (4) and 4(2)\n",
    "        Each matrix will be initialized with random values\n",
    "        \"\"\"\n",
    "        self.weights=[np.random.randn(y,x) for x,y in zip(size[:-1],size[1:])]\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Perform a feed forward computation\n",
    "        Parameters\n",
    "        ---\n",
    "        input: data to be fed to the network with\n",
    "        shape: (input_shape, batch_size)\n",
    "        Returns\n",
    "        ---\n",
    "        a: ouptut activation (output_shape, batch_size)\n",
    "        pre_activations: list of pre-activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        activations: list of activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        '''\n",
    "        a = input\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        # TODO\n",
    "        # what does the zip function do?\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a  = activation(z)\n",
    "            pre_activations.append(z)\n",
    "            activations.append(a)\n",
    "        return a, pre_activations, activations\n",
    "\n",
    "    \"\"\"\n",
    "    Resources:\n",
    "    https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "    https://hmkcode.github.io/ai/backpropagation-step-by-step/\n",
    "    \"\"\"\n",
    "    def compute_deltas(self, pre_activations, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes a list containing the values of delta for each layer using\n",
    "        a recursion\n",
    "        Parameters:\n",
    "        ---\n",
    "        pre_activations: list of of pre-activations. each corresponding to a layer\n",
    "        y_true: ground truth values of the labels\n",
    "        y_pred: prediction values of the labels\n",
    "        Returns:\n",
    "        ---\n",
    "        deltas: a list of deltas per layer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize array to store the derivatives\n",
    "        deltas = [0] * (len(self.size) - 1)\n",
    "\n",
    "        #TODO\n",
    "        # Calculate the delta for each layer\n",
    "        # This is the first step in calculating the derivative\n",
    "        #The last layer is calculated as derivative of cost function *  derivative of sigmoid ( pre-activations of last layer )\n",
    "        deltas[-1] = cost_function_prime(y_true, y_pred)*activation(pre_activations, derivative=True)\n",
    "\n",
    "        #TODO\n",
    "        # Recursively calculate delta for each layer from the previous layer\n",
    "        for l in range(len(deltas) - 2, -1, -1):\n",
    "            pass\n",
    "            # deltas of layer l depend on the weights of layer l and l+1 and on the sigmoid derivative of the pre-activations of layer l\n",
    "            # Note that we use a dot product when multipying the weights and the deltas\n",
    "            # Check their shapes to ensure that their shapes conform to the requiremnts (You may need to transpose some of the matrices)\n",
    "            # The final shape of deltas of layer l must be the same as that of the activations of layer l\n",
    "            # Check if this is true\n",
    "            deltas[l] = np.dot(self.weights[l+1].T, deltas[l+1])*activation(pre_activations[l], derivative=True)\n",
    "        return deltas\n",
    "\n",
    "    def backpropagate(self, deltas, pre_activations, activations):\n",
    "        \"\"\"\n",
    "        Applies back-propagation and computes the gradient of the loss\n",
    "        w.r.t the weights and biases of the network\n",
    "        Parameters:\n",
    "        ---\n",
    "        deltas: list of deltas computed by compute_deltas\n",
    "        pre_activations: a list of pre-activations per layer\n",
    "        activations: a list of activations per layer\n",
    "        Returns:\n",
    "        ---\n",
    "        dW: list of gradients w.r.t. the weight matrices of the network\n",
    "        db: list of gradients w.r.t. the biases (vectors) of the network\n",
    "\n",
    "        \"\"\"\n",
    "        dW = []\n",
    "        db = []\n",
    "        deltas = [0] + deltas\n",
    "        for l in range(1, len(self.size)):\n",
    "            # TODO\n",
    "            # Compute the derivatives of the weights and the biases from the delta values calculated earlier\n",
    "            # dW_temp depends on the activations of layer l-1 and the deltas of layer l\n",
    "            # dB_temp depends only on the deltas of layer l\n",
    "            # Again be careful of the dimensions and ensure that the dW matrix has the same shape as W\n",
    "            dW_temp = np.dot(deltas[l], activations[l-1].T)\n",
    "            dB_temp = deltas[l]\n",
    "            dW.append(dW_temp)\n",
    "            db.append(np.expand_dims(dB_temp.mean(axis=1), 1))\n",
    "        return dW, db\n",
    "\n",
    "    def plot_loss(self,epochs,train,test):\n",
    "        \"\"\"\n",
    "        Plots the loss function of the train test data measured every epoch\n",
    "        Parameters:\n",
    "        ---\n",
    "        epochs: number of epochs for training\n",
    "        train: list of losses on the train set measured every epoch\n",
    "        test: list of losses on the test set measured every epoch\n",
    "        \"\"\"\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.title('Training Cost (loss)')\n",
    "        plt.plot(range(epochs),train)\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('Test Cost (loss)')\n",
    "        plt.plot(range(epochs),test)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the network using the gradients computed by back-propagation\n",
    "        Splits the data in train and validation splits\n",
    "        Processes the training data by batches and trains the network using batch gradient descent\n",
    "        Parameters:\n",
    "        ---\n",
    "        X: input data\n",
    "        y: input labels\n",
    "        batch_size: number of data points to process in each batch\n",
    "        epochs: number of epochs for the training\n",
    "        learning_rate: value of the learning rate\n",
    "        validation_split: percentage of the data for validation\n",
    "        print_every: the number of epochs by which the network logs the loss and accuracy metrics for train and validations splits\n",
    "        plot_every: the number of epochs by which the network plots the decision boundary\n",
    "\n",
    "        Returns:\n",
    "        ---\n",
    "        history: dictionary of train and validation metrics per epoch\n",
    "            train_acc: train accuracy\n",
    "            test_acc: validation accuracy\n",
    "            train_loss: train loss\n",
    "            test_loss: validation loss\n",
    "        This history is used to plot the performance of the model\n",
    "        \"\"\"\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "\n",
    "        # TODO\n",
    "        # Read about the train_test_split function\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n",
    "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "        epoch_iterator = range(epochs)\n",
    "\n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train.T, y_train.T)\n",
    "            x_train, y_train = x_train.T, y_train.T\n",
    "\n",
    "            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "            # weight update\n",
    "\n",
    "            # TODO\n",
    "            # What does the enumerate function do?\n",
    "            for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                pass\n",
    "                # TODO\n",
    "                self.weights[i] = self.weights[i] - learning_rate*dw_epoch\n",
    "                self.biases[i] = self.biases[i] - learning_rate*db_epoch\n",
    "                # Update the weights using the backpropagation algorithm implemented earlier\n",
    "                # W = W - learning_rate * derivatives (dW)\n",
    "                # b = b - learning_rate * derivatives (db)\n",
    "                # self.weights =\n",
    "                # self.biases =\n",
    "\n",
    "            history_train_losses.append(np.mean(train_losses))\n",
    "            history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "            history_test_losses.append(np.mean(test_losses))\n",
    "            history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "\n",
    "            if e % print_every == 0:\n",
    "                print('Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '.format(\n",
    "                    e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3),\n",
    "                    np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n",
    "\n",
    "        self.plot_loss(epochs,train_loss,test_loss)\n",
    "\n",
    "        history = {'epochs': epochs,\n",
    "                   'train_loss': history_train_losses,\n",
    "                   'train_acc': history_train_accuracies,\n",
    "                   'test_loss': history_test_losses,\n",
    "                   'test_acc': history_test_accuracies\n",
    "                   }\n",
    "        return history\n",
    "\n",
    "    def predict(self, a):\n",
    "        '''\n",
    "        Use the current state of the network to make predictions\n",
    "        Parameters:\n",
    "        ---\n",
    "        a: input data, shape: (input_shape, batch_size)\n",
    "        Returns:\n",
    "        ---\n",
    "        predictions: vector of output predictions\n",
    "        '''\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z)\n",
    "        predictions = (a > 0.5).astype(int)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Author: Ahmed BESBES\n",
    "# <ahmed.besbes@hotmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import neural_net\n",
    "from neural_net import NeuralNetwork\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [784, 70000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-83b5a29caa28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\ML_Recruitment-master\\ML_Recruitment-master\\neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, batch_size, epochs, learning_rate, validation_split, print_every)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;31m# TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# Read about the train_test_split function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2094\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [784, 70000]"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([784, 15, 10])\n",
    "nn.train(X, y, 10, 20, learning_rate=0.1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
